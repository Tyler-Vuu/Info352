<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="author" content="Tyler Vuu">
    <meta name="description" content="Home page for storytree">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
    <link rel="icon" href="img/storytree.ico" type="image/x-icon">
    <title>Info 352 Final</title>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Courgette&display=swap">
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <nav>
        <ul>
            <li><img src="img/iSchool.png" alt="iSchool"></li>
            <li>
                <h2 className="navbar-title">Home Page</h2>
            </li>
            <li>
                <a href="HomePage.html">
                    <!--<img class="smallimage" src="img/Home.png" alt="Home" aria-label="Home">-->
                    <button>Home</button>
                </a>
            </li>
            <li>
                <a href="Race.html">
                    <!--<img class="smallimage" src="img/Home.png" alt="Home" aria-label="Home">-->
                    <button>Race</button>
                </a>
            </li>
            <li>
                <a href="Gender.html">
                    <!--<img class="smallimage" src="img/Home.png" alt="Home" aria-label="Home">-->
                    <button>Gender</button>
                </a>
            </li>
            <li>
                <a href="Combating.html">
                    <!--<img class="smallimage" src="img/Home.png" alt="Home" aria-label="Home">-->
                    <button>Potential Solutions</button>
                </a>
            </li>
            <li>
                <a href="Conclusion.html">
                    <!--<img class="smallimage" src="img/Home.png" alt="Home" aria-label="Home">-->
                    <button>Conclusion</button>
                </a>
            </li>
            <li>
                <a href="Sources.html">
                    <!--<img class="smallimage" src="img/Home.png" alt="Home" aria-label="Home">-->
                    <button>Sources</button>
                </a>
            </li>
        </ul>
    </nav>


    <main>
        <h1>Race and AI</h1>
        <p>In today’s world, AI can mirror society's complexities, including the issue of ethnic
            bias. Making AI that is fair and does not discriminate against people of their background
            is a big goal. When it comes to addressing bias and promoting diversity, AI developers have
            to face complex challenges. A few years ago, the problem was AI algorithms not generating
            enough diversity. When the users searched for pictures of CEOs, the output was mainly pictures
            of white businessmen. When searching for angry men, the result shows more images of black men,
            which was a clear indication of gender and racial bias. This bias resulted from poorly trained
            data used by these algorithms, which historically had more examples of white men in such roles,
            and black men in negative situations, leading the AI to reinforce social stereotypes.
        </p>
        <p>
            In response to such issues, companies like OpenAI and Google have made efforts to adjust
            their algorithms to be more inclusive and diverse. A change these companies made was to develop
            techniques to insert terms reflecting diversity, such as "Asian" or "female," into regular prompts.
            These adjustments aimed to counteract the inherent bias in the training data and produce results that
            better reflect the diversity of the real world. However, these well-intentioned efforts can sometimes
            backfire, leading to outcomes that are even more inappropriate, an example was with Google's Gemini AI
            generating images for a 1943 German soldier. Given the prompt, the AI instead generated four picture
            options with a “White man”, an “Asian woman”, a “Black man”, and a “Latino woman”, which was
            intentionally boosting diversity in output but was leading to more problems. This outcome went
            controversial and was accused of inaccurate images. The incident with Google's AI proved that it is in
            fact an ongoing challenge in AI development.
            https://arstechnica.com/information-technology/2024/02/googles-hidden-ai-diversity-prompts-lead-to-outcry-over-historically-inaccurate-images/
        </p>
        <p>
            People tend to overestimate the ability of AI, thinking they are less biased than human’s
            decision making. In fact, AI calculating through algorithms can discriminate and further amplify
            bias. Another example is algorithms used as policing tools that are utilized to predict where crimes
            are likely to occur or who is likely to commit them. These tools come in two types: location-based
            algorithms,
            which predict crime hotspots, and person-based tools, which predict individuals' likelihood of committing
            crimes. Critics argue that these tools are inherently racist because they are fed with biased arrest history
            data, which disproportionately targets Black and minority communities. Although the initial belief that
            these
            algorithms could bring objectivity and fairness to policing, evidence suggests that they may reinforce
            existing social inequities.
            https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/
        </p>
        <p>
            Considering AI's significant impact on people and its indispensability in our daily lives, how can we ensure
            that it is not trapped in a limited view of historic biases but instead shapes our future in a more
            inclusive and equitable manner?
        </p>
    </main>

    <footer>
        <p>Info 352 Final Project</p>
        <p>By Tyler Vuu and Teresa Wang</p>
    </footer>
</body>